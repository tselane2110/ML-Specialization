{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/tselane2110/ML-Specialization/blob/main/Logistic_Regression_Implementation.ipynb",
      "authorship_tag": "ABX9TyPfXstTmJQy0rRYfk8VQOQ2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tselane2110/ML-Specialization/blob/main/Logistic_Regression_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression without Regularization"
      ],
      "metadata": {
        "id": "d6KyKJ0LLRsg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "cZeB6VqfJryi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "class LogisticRegression:\n",
        "  def fit_data(self, x, y):\n",
        "    \"\"\"\n",
        "    self.x -> training data (features)\n",
        "    self.y -> training data (labels/target-values/actual-values)\n",
        "    self.m -> no of training examples\n",
        "    self.n -> no of features\n",
        "    self.w -> initializing weights\n",
        "    self.b -> initializing bias\n",
        "    self.y_hat -> initializing prediction's list\n",
        "    self.cost_rec -> record of cost function of all iterations\n",
        "    self.min_iterations -> minimum number of iterations for our gradient descent\n",
        "    self.prediction_threshold -> if the output of sigmoid function is greater than or equal to this threshold, then y_hat will be 1, else 0\n",
        "    \"\"\"\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "    self.m = len(self.x)\n",
        "    self.n = self.x.shape[1]\n",
        "    self.w = np.zeros(self.n)\n",
        "    self.b = 0\n",
        "    self.y_hat = []\n",
        "    self.cost_rec = []\n",
        "    self.min_iterations = 40\n",
        "    self.prediction_threshold = 0.5\n",
        "\n",
        "    # calling recursive gradient descent\n",
        "    self.recursiveGD()\n",
        "\n",
        "  def predict_training(self, x):\n",
        "    y_hat = []\n",
        "    z = []\n",
        "    e = math.e\n",
        "    for i in range(len(x)):\n",
        "      z.append(np.dot(self.w, x[i]) + self.b)\n",
        "    for i in range(len(z)):\n",
        "      y_hat.append((1/(1+(pow(e, -(z[i]))))))\n",
        "    return y_hat\n",
        "\n",
        "  def predict(self, x):\n",
        "    y_hat = []\n",
        "    z = []\n",
        "    e = math.e\n",
        "    for i in range(len(x)):\n",
        "      z.append(np.dot(self.w, x[i]) + self.b)\n",
        "    for i in range(len(z)):\n",
        "      y_hat.append((1/(1+(pow(e, -(z[i]))))))\n",
        "    for i in range(len(y_hat)):\n",
        "      if y_hat[i] >= self.prediction_threshold:\n",
        "        y_hat[i] = 1\n",
        "      else:\n",
        "        y_hat[i] = 0\n",
        "    return y_hat\n",
        "\n",
        "  def calculate_loss(self, y_hat, y):\n",
        "    loss = []\n",
        "    for i in range(len(y_hat)):\n",
        "      a = -y[i] * math.log(y_hat[i])\n",
        "      b = (1 - y[i]) * math.log(1 - y_hat[i])\n",
        "      loss.append(a-b)\n",
        "    return loss\n",
        "    \"\"\"\n",
        "    loss = []\n",
        "    for i in range(len(y_hat)):\n",
        "      if y[i] == 1:\n",
        "        loss.append(-math.log(y_hat[i]))\n",
        "      else:\n",
        "        loss.append(-math.log(1 - y_hat[i]))\n",
        "    return loss\"\"\"\n",
        "\n",
        "  def calculate_cost(self, loss):\n",
        "    cost = 0\n",
        "    for i in range(len(loss)):\n",
        "      cost += loss[i]\n",
        "    cost += (1/self.m)\n",
        "    return cost\n",
        "\n",
        "  def abs_diff(self, a, b):\n",
        "    # abs_diff stands for absolute difference\n",
        "    result = abs(a-b)\n",
        "    return result\n",
        "\n",
        "  def stopping_criteria_check(self, threshold):\n",
        "    '''\n",
        "    Need to check that the cost function for the last 20 iterations has converged\n",
        "    Also verifying that we have calculated the cost atleast 40 times, idk for what reason but yea (I do know, i mean that's the min number\n",
        "    of iterations Im aiming for)\n",
        "    '''\n",
        "    cost = self.cost_rec\n",
        "\n",
        "    if len(cost) >= self.min_iterations :\n",
        "      for i in range(-21, -1, 1):\n",
        "        if self.abs_diff(cost[i], cost[i+1]) > threshold:\n",
        "          return 1\n",
        "        else:\n",
        "          return 0\n",
        "\n",
        "    else:\n",
        "      return 1\n",
        "\n",
        "  def plot_learningCurve(self):\n",
        "    # printing the total number of iterations\n",
        "    print(\"total number of iterations: \", len(self.cost_rec))\n",
        "\n",
        "    # plotting the graph\n",
        "    plt.plot(range(len(self.cost_rec)), self.cost_rec, label='Learning Curve')  # Plot cost_function vs # of iterations\n",
        "    plt.xlabel('Number of Iterations')\n",
        "    plt.ylabel('Cost Function J(w, b)')\n",
        "    plt.title('Learning Curve ')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "  def calculate_error_w(self, j):\n",
        "    error_wj = 0\n",
        "    for i in range(len(self.y_hat)):\n",
        "      error_wj += (self.y_hat[i] - self.y[i]) * self.x[i][j]\n",
        "    return error_wj\n",
        "\n",
        "  def calculate_error_b(self):\n",
        "    error_b = 0\n",
        "    for i in range(len(self.y_hat)):\n",
        "      error_b += (self.y_hat[i] - self.y[i])\n",
        "    return error_b\n",
        "\n",
        "  def recursiveGD(self):\n",
        "    # making predictions\n",
        "    self.y_hat = self.predict_training(self.x)\n",
        "\n",
        "    # calculate loss\n",
        "    loss = self.calculate_loss(self.y_hat, self.y)\n",
        "\n",
        "    # caculate cost\n",
        "    self.cost_rec.append(self.calculate_cost(loss))\n",
        "\n",
        "    # calling recursive gradient descent\n",
        "    self.gradient_descent()\n",
        "\n",
        "\n",
        "  def gradient_descent(self):\n",
        "    alpha = 0.01\n",
        "    threshold = 1e-3 # 0.001\n",
        "\n",
        "    # stopping criteria\n",
        "    sc = self.stopping_criteria_check(threshold)\n",
        "    if sc == 0:\n",
        "      self.plot_learningCurve()\n",
        "      return\n",
        "\n",
        "    # updating w\n",
        "    for j in range(len(self.w)):\n",
        "      error = self.calculate_error_w(j)\n",
        "      self.w[j] = self.w[j] - (alpha * (1/self.m) * error)\n",
        "\n",
        "    # updating b\n",
        "    error = self.calculate_error_b()\n",
        "    self.b = self.b - (alpha * (1/self.m) * error)\n",
        "\n",
        "    # calling recursive gradient descent\n",
        "    self.recursiveGD()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Dataset to test the model"
      ],
      "metadata": {
        "id": "dgely7z4qXEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# change the max number of rows/columns to be displayed in the cell\n",
        "pd.set_option(\"display.max_rows\", 500)\n",
        "\n",
        "# importing dataset\n",
        "df=pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/MachineLearningSpecialization_AndrewNg/Telco-Customer-Churn.csv\")\n",
        "\n",
        "# encoding\n",
        "df[df.columns]=df[df.columns].apply(lambda col:pd.Categorical(col).codes)\n",
        "\n",
        "# dropping extra columns\n",
        "df=df.drop(columns=[\"gender\",\"TotalCharges\", \"StreamingMovies\", \"StreamingTV\", \"InternetService\", \"MultipleLines\",\\\n",
        "                    \"PhoneService\", \"MonthlyCharges\"])\n",
        "\n",
        "# distributing the data into independent and dependent variables:\n",
        "x=df[['SeniorCitizen', 'Partner', 'Dependents', 'tenure',\n",
        "       'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport',\n",
        "       'Contract', 'PaperlessBilling', 'PaymentMethod']]\n",
        "y=df[\"Churn\"]\n",
        "\n",
        "# splitting data into train/test\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0)"
      ],
      "metadata": {
        "id": "wGxezDeCKQGs"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape, y_train.shape"
      ],
      "metadata": {
        "id": "fVQ_P5uPr3cC",
        "outputId": "9f14f1af-8c9b-4818-b65f-c0ab8a2b4a87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4930, 11), (4930,))"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing our Logistic Regression model without regularization"
      ],
      "metadata": {
        "id": "KtbFqO8RriMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression()"
      ],
      "metadata": {
        "id": "tdeu1lvIrcmk"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit_data(np.array(x_train), np.array(y_train))"
      ],
      "metadata": {
        "id": "LCBKPJj2rqO9",
        "outputId": "4974edb1-8489-498a-d118-44de84775258",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RecursionError",
          "evalue": "maximum recursion depth exceeded while calling a Python object",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-cb30d523aef7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-36-15e923d0d903>\u001b[0m in \u001b[0;36mfit_data\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# calling recursive gradient descent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecursiveGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mpredict_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-15e923d0d903>\u001b[0m in \u001b[0;36mrecursiveGD\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;31m# calling recursive gradient descent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-15e923d0d903>\u001b[0m in \u001b[0;36mgradient_descent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;31m# calling recursive gradient descent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecursiveGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "... last 2 frames repeated, from the frame below ...\n",
            "\u001b[0;32m<ipython-input-36-15e923d0d903>\u001b[0m in \u001b[0;36mrecursiveGD\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;31m# calling recursive gradient descent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded while calling a Python object"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_hat = model.predict(x_test)\n",
        "error = y_hat - y_test\n",
        "error"
      ],
      "metadata": {
        "id": "vtC0sa7FrtF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "confusion_matrix= pd.crosstab(y_test,y_hat, rownames=['actual'],colnames=['predicted'])\n",
        "accuracy_logistic=accuracy_score(y_test, y_hat)\n",
        "print(confusion_matrix, accuracy_logistic)"
      ],
      "metadata": {
        "id": "pzU8_qPys-xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dRpCP8KJy95S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}